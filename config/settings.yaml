## Main configuration settings for the application

#Scraping methods
scraping_methods:
  - selenium

defaults:
  max-tweets: 100
  timeout: 30
  retry-attempts: 3
  delay_between-requests: 2 # in seconds

#Selenium settings (fyi i use mac so i used its agent code)
selenium:
  headless: true
  window-size: "1920,1080"
  user_agent: "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
  implicit_wait: 10 # in seconds
  page_load_timeout: 30 # in seconds
  max_scrolls: 5
  scroll_pause_time: 3
  chrome_options: # got it from ai / sonnet
    - "--no-sandbox"
    - "--disable-dev-shm-usage"
    - "--disable-gpu"
    - "--disable-extensions"
    - "--disable-plugins"

# Data Processing
data_processing:
  remove_duplicates: true
  clean_text: true
  extract_hashtags: true
  extract_mentions: true
  extract_urls: true
  sentiment_analysis: false # i have just added basic impl so false for cli app

# Export settings
export:
  formats:
    - csv
    - json
  include_metadata: true
  datetime_format: "%Y-%m-%d %H:%M:%S"
  encoding: "utf-8"

#Caching :) / Not used for now
cache:
  enabled: true
  expire_hours: 24
  directory: "data/cache"

#Logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file_rotation: "10MB"
  backup_count: 5

# Rate Limiting
rate_limiting:
  enabled: true
  requests_per_minute: 30
  burst_limit: 10
  backoff_factor: 2 # Exponential backoff factor

# Error Handling
error_handling:
  enabled: true
  max_retries: 5
  retry_delay: 2 # in seconds
  exponential_backoff: true
  ignore_errors:
    - "rate_limit"
    - "not_found"
